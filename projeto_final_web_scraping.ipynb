{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "802f65fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_period_dtype\n",
    "\n",
    "import asyncio\n",
    "from typing import List, Dict, Optional\n",
    "from playwright.async_api import async_playwright, Page\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import datetime as dt\n",
    "from typing import Optional, Tuple\n",
    "import requests\n",
    "import duckdb\n",
    "\n",
    "import asyncio, csv, re\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin\n",
    "from playwright.async_api import async_playwright, Page\n",
    "from datetime import datetime, timezone\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8e959f",
   "metadata": {},
   "source": [
    "# üß† Projeto: Enriquecimento de Base Anal√≠tica com Web Scraping e API Financeira\n",
    "\n",
    "## üéØ Contexto\n",
    "\n",
    "Uma **fintech de investimentos** precisa enriquecer sua base anal√≠tica com informa√ß√µes externas do mercado para apoiar decis√µes estrat√©gicas.\n",
    "Como Engenheira de Dados, foi desenvolvido um **pipeline de dados** que coleta informa√ß√µes p√∫blicas de **not√≠cias** e **s√©ries financeiras**, armazena localmente em um **banco DuckDB**, e permite posterior explora√ß√£o via SQL e dashboards.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Objetivo\n",
    "\n",
    "Construir um pipeline completo de **coleta, transforma√ß√£o e carga (ETL)** que una:\n",
    "\n",
    "* **Web Scraping** de not√≠cias econ√¥micas e geopol√≠ticas (BBC News);\n",
    "* **API P√∫blica** de dados financeiros (FRED e CoinGecko);\n",
    "* **Integra√ß√£o anal√≠tica** em banco local **DuckDB**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Stack Utilizada\n",
    "\n",
    "| Etapa         | Tecnologia                        | Descri√ß√£o                                          |\n",
    "| ------------- | --------------------------------- | -------------------------------------------------- |\n",
    "| Coleta Web    | `Playwright` + `asyncio`          | Scraping ass√≠ncrono de p√°ginas de not√≠cias da BBC  |\n",
    "| Coleta API    | `requests`, `pandas`              | Consumo de APIs FRED (Federal Reserve) e CoinGecko |\n",
    "| Armazenamento | `DuckDB`                          | Banco anal√≠tico local com tr√™s tabelas             |\n",
    "| Ambiente      | `Python 3.9+`, `Jupyter Notebook` | Execu√ß√£o e an√°lise                                 |\n",
    "| Persist√™ncia  | `.duckdb`, `.parquet`, `.csv`     | Formatos intermedi√°rios                            |\n",
    "\n",
    "---\n",
    "\n",
    "## üåê Fontes de Dados\n",
    "\n",
    "### üîπ Not√≠cias (Web Scraping ‚Äì BBC News)\n",
    "\n",
    "* Fonte: [BBC News ‚Äì US-Canada](https://www.bbc.com/news/us-canada)\n",
    "* Coletadas **100 not√≠cias** contendo t√≠tulo, resumo, link e data de coleta.\n",
    "* Campos armazenados:\n",
    "\n",
    "  ```\n",
    "  ['title', 'url', 'summary', 'collected_at']\n",
    "  ```\n",
    "* Objetivo: capturar contexto geopol√≠tico e eventos com impacto em mercados.\n",
    "\n",
    "### üîπ S√©ries Financeiras (APIs P√∫blicas)\n",
    "\n",
    "| Fonte     | S√©rie          | Descri√ß√£o                                   | Per√≠odo  |\n",
    "| --------- | -------------- | ------------------------------------------- | -------- |\n",
    "| FRED      | `DCOILBRENTEU` | Pre√ßo di√°rio do petr√≥leo Brent (USD/barril) | 6+ meses |\n",
    "| FRED      | `DEXUSUK`      | Taxa USD/GBP (invertida para GBP/USD)       | 6+ meses |\n",
    "| CoinGecko | `BTC/USD`      | Cota√ß√£o di√°ria do Bitcoin                   | 6+ meses |\n",
    "\n",
    "Os dados foram padronizados em base di√°ria cont√≠nua, com c√°lculo de retornos em janelas de 1, 3 e 5 dias (`r1`, `r3`, `r5`).\n",
    "\n",
    "---\n",
    "\n",
    "## üóÑÔ∏è Modelagem de Dados no DuckDB\n",
    "\n",
    "### Tabelas criadas:\n",
    "\n",
    "| Tabela          | Descri√ß√£o                       | Principais Campos                          |\n",
    "| --------------- | ------------------------------- | ------------------------------------------ |\n",
    "| **prices**      | S√©ries hist√≥ricas dos ativos    | `instr`, `date`, `close`, `r1`, `r3`, `r5` |\n",
    "| **news_bbc**    | Not√≠cias coletadas via scraping | `title`, `url`, `summary`, `collected_at`, `published_at`, `published_text`  |\n",
    "| **instruments** | Metadados dos instrumentos      | `instr_id`, `symbol`, `name`, `class`      |\n",
    "\n",
    "```sql\n",
    "-- Exemplo de schema no DuckDB\n",
    "DESCRIBE prices;\n",
    "DESCRIBE news_bbc;\n",
    "DESCRIBE instruments;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Resultados\n",
    "\n",
    "* **100 not√≠cias** coletadas da BBC News.\n",
    "* **3 instrumentos** (Brent, GBP/USD, BTC/USD) com **211 dias** de dados cada.\n",
    "* **3 tabelas anal√≠ticas** armazenadas no DuckDB (`prices`, `news_bbc`, `instruments`).\n",
    "* Pipeline totalmente reprodut√≠vel e modular, pronto para expans√£o com novos t√≥picos ou ativos.\n",
    "\n",
    "---\n",
    "\n",
    "## üßæ Estrutura Final\n",
    "\n",
    "```\n",
    "üìÇ projeto_etl_fintech/\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ market.duckdb                 # Banco anal√≠tico local\n",
    "‚îú‚îÄ‚îÄ prices.parquet                # Dados de pre√ßos\n",
    "‚îú‚îÄ‚îÄ bbc_us_canada_latest_updates.csv       # Not√≠cias coletadas\n",
    "‚îú‚îÄ‚îÄ projeto_final_web_scraping.ipynb            # Notebook principal\n",
    "‚îî‚îÄ‚îÄ requirements.txt              # Depend√™ncias fixas\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Conclus√£o\n",
    "\n",
    "O projeto integra dados n√£o estruturados (not√≠cias) e estruturados (s√©ries econ√¥micas), simulando um fluxo real de engenharia de dados.\n",
    "Com as tabelas organizadas no DuckDB, √© poss√≠vel executar consultas SQL r√°pidas e realizar an√°lises temporais sobre o impacto de eventos geopol√≠ticos nos ativos financeiros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa98c8b1",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d5c52ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "DUCKDB_PATH = os.getenv(\"DUCKDB_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ed4ed77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Configs\n",
    "# =========================\n",
    "# 100 √∫ltimas not√≠cias do BBC US & Canada\n",
    "\n",
    "START_URL = os.getenv(\"NEWS_SOURCE\")\n",
    "TARGET = 100\n",
    "BASE = \"https://www.bbc.com\"\n",
    "# varredura come√ßa por v√°rias se√ß√µes de /news (aumenta cobertura)\n",
    "START_URLS = [\n",
    "    \"https://www.bbc.com/news\",\n",
    "    \"https://www.bbc.com/news/world\",\n",
    "    \"https://www.bbc.com/news/uk\",\n",
    "    \"https://www.bbc.com/news/business\",\n",
    "    \"https://www.bbc.com/news/technology\",\n",
    "    \"https://www.bbc.com/news/science_and_environment\",\n",
    "    \"https://www.bbc.com/news/entertainment_and_arts\",\n",
    "]\n",
    "OUTCSV = \"bbc_us_canada_latest_updates.csv\"\n",
    "\n",
    "# Somente artigos v√°lidos:\n",
    "ART_RE = re.compile(r\"^https?://(?:www\\.)?bbc\\.com/news/articles/[A-Za-z0-9]+(?:[/?#]|$)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9d31a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Configs\n",
    "# =========================\n",
    "# Per√≠odo alvo (>= 6 meses); pego ~210 dias\\\n",
    "\n",
    "END = dt.date.today()\n",
    "START = END - dt.timedelta(days=210)  # ~7 meses\n",
    "\n",
    "FRED_API_KEY = os.getenv(\"FRED_API_KEY\")  # .env\n",
    "FRED_BASE = \"https://api.stlouisfed.org/fred/series/observations\"\n",
    "FRED_SERIES = {\n",
    "    \"BRENT\": \"DCOILBRENTEU\",  # Brent Europe, di√°rio\n",
    "    \"GBPUSD\": \"DEXUSUK\",      # Taxa USD/GBP di√°ria \n",
    "}\n",
    "\n",
    "COINGECKO_BASE = \"https://api.coingecko.com/api/v3\"\n",
    "COINGECKO_COIN = \"bitcoin\"\n",
    "COINGECKO_VS = \"usd\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ab16f4",
   "metadata": {},
   "source": [
    "## Web scraping de not√≠cias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41180138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Um \"listing\" (p√°gina de lista) precisa estar em /news e N√ÉO ser um artigo:\n",
    "def is_news_listing_url(url: str) -> bool:\n",
    "    try:\n",
    "        u = urlparse(url)\n",
    "        if u.netloc not in (\"www.bbc.com\", \"bbc.com\"): return False\n",
    "        if not u.path.startswith(\"/news\"): return False\n",
    "        if \"/news/articles/\" in u.path:  # artigo (n√£o √© listing)\n",
    "            return False\n",
    "        # evita √°reas sabidamente fora do news feed tradicional\n",
    "        if any(seg in u.path for seg in (\"/reel/\", \"/future/\", \"/innovation/\", \"/culture/\")):\n",
    "            return False\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# ---------- Utils ----------\n",
    "def abs_url(href): \n",
    "    return urljoin(BASE, href or \"\")\n",
    "\n",
    "def _now_iso():\n",
    "    return datetime.now(timezone.utc).astimezone().isoformat()\n",
    "\n",
    "def _norm_iso(dt: str) -> str:\n",
    "    if not dt: return \"\"\n",
    "    try:\n",
    "        return datetime.fromisoformat(dt.replace(\"Z\",\"+00:00\")).astimezone(timezone.utc).isoformat()\n",
    "    except Exception:\n",
    "        return dt.strip()\n",
    "\n",
    "# ---------- Cookies ----------\n",
    "async def accept_cookies(page: Page):\n",
    "    for sel in (\n",
    "        '[data-testid=\"cookie-banner\"] button:has-text(\"Accept\")',\n",
    "        'button:has-text(\"I Agree\")',\n",
    "        'button:has-text(\"Agree\")',\n",
    "        '#bbccookies-continue-button',\n",
    "    ):\n",
    "        b = page.locator(sel).first\n",
    "        if await b.count() and await b.is_visible():\n",
    "            await b.click(); break\n",
    "\n",
    "# ---------- EXTRA√á√ÉO: links de artigos (news/articles/ID) ----------\n",
    "async def extract_article_urls_on_page(page: Page):\n",
    "    js = r\"\"\"\n",
    "    () => {\n",
    "      const BASE = 'https://www.bbc.com';\n",
    "      const root = document.querySelector('main') || document;\n",
    "      const anchors = Array.from(root.querySelectorAll('a[href*=\"/news/articles/\"]'));\n",
    "      const seen = new Set(), urls = [];\n",
    "      for (const a of anchors) {\n",
    "        const href = a.getAttribute('href') || '';\n",
    "        try {\n",
    "          const url = new URL(href, BASE).toString();\n",
    "          if (!seen.has(url)) { seen.add(url); urls.push(url); }\n",
    "        } catch {}\n",
    "      }\n",
    "      return urls;\n",
    "    }\n",
    "    \"\"\"\n",
    "    raw = await page.evaluate(js)\n",
    "    urls = [u for u in raw if ART_RE.search(u)]\n",
    "    print(f\"[{page.url}] artigos v√°lidos nesta p√°gina: {len(urls)}\")\n",
    "    return urls\n",
    "\n",
    "# ---------- DESCOBRIR novas p√°ginas de listagem (paginadores + subse√ß√µes) ----------\n",
    "async def discover_listing_pages(page: Page):\n",
    "    \"\"\"\n",
    "    Encontra:\n",
    "      - links que sugerem pagina√ß√£o (?page=, /page/N/, /page-N) dentro de /news\n",
    "      - links de subse√ß√£o ainda em /news (evitando reel/future/innovation/culture)\n",
    "    \"\"\"\n",
    "    js = r\"\"\"\n",
    "    () => {\n",
    "      const BASE = 'https://www.bbc.com';\n",
    "      const anchors = Array.from(document.querySelectorAll('a[href]'));\n",
    "      const urls = [];\n",
    "      for (const a of anchors) {\n",
    "        const href = a.getAttribute('href') || '';\n",
    "        try {\n",
    "          const u = new URL(href, BASE).toString();\n",
    "          urls.push(u);\n",
    "        } catch {}\n",
    "      }\n",
    "      return urls;\n",
    "    }\n",
    "    \"\"\"\n",
    "    all_urls = await page.evaluate(js)\n",
    "    out = set()\n",
    "    for u in all_urls:\n",
    "        if not is_news_listing_url(u): \n",
    "            continue\n",
    "        # heur√≠stica: priorizar sinais de pagina√ß√£o OU subse√ß√µes de /news\n",
    "        if re.search(r\"[?&]page=\\d+\", u) or re.search(r\"/page/\\d+/?\", u) or re.search(r\"/page-\\d+/?\", u):\n",
    "            out.add(u)\n",
    "        else:\n",
    "            # subse√ß√µes /news/... (sem page) tamb√©m ajudam a diversificar\n",
    "            out.add(u)\n",
    "    # limita o volume por p√°gina para evitar explos√£o de crawling\n",
    "    # prioriza paginadores expl√≠citos primeiro\n",
    "    pagers = [u for u in out if re.search(r\"(?:[?&]page=\\d+|/page/\\d+|/page-\\d+)\", u)]\n",
    "    subsecs = [u for u in out if u not in pagers]\n",
    "    # devolve at√© 20 candidatos, pagers primeiro\n",
    "    ordered = pagers[:15] + subsecs[:5]\n",
    "    print(f\"[{page.url}] novos listings candidatos: {len(ordered)}\")\n",
    "    return ordered\n",
    "\n",
    "# ---------- ENRIQUECIMENTO POR REQUEST ----------\n",
    "_JSONLD_RE = re.compile(\n",
    "    r'<script[^>]+type=[\"\\']application/ld\\+json[\"\\'][^>]*>(.*?)</script>',\n",
    "    re.I | re.S\n",
    ")\n",
    "\n",
    "def _extract_meta(html: str, name: str, attr=\"content\"):\n",
    "    pat = rf'<meta[^>]+(?:property|name)=[\"\\']{re.escape(name)}[\"\\'][^>]*{attr}=[\"\\']([^\"\\']+)[\"\\']'\n",
    "    m = re.search(pat, html, re.I)\n",
    "    return m.group(1).strip() if m else \"\"\n",
    "\n",
    "def _extract_tag_text(html: str, tag: str):\n",
    "    m = re.search(rf'<{tag}[^>]*>(.*?)</{tag}>', html, re.I | re.S)\n",
    "    if not m: return \"\"\n",
    "    txt = re.sub(r\"<[^>]+>\", \" \", m.group(1))\n",
    "    return re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "\n",
    "def _first_nonempty(*vals):\n",
    "    for v in vals:\n",
    "        if v and str(v).strip():\n",
    "            return str(v).strip()\n",
    "    return \"\"\n",
    "\n",
    "def _from_jsonld_block(obj):\n",
    "    t = str(obj.get(\"@type\"))\n",
    "    if not any(k in t for k in (\"Article\",\"NewsArticle\",\"LiveBlogPosting\")):\n",
    "        return (\"\",\"\",\"\")\n",
    "    title = _first_nonempty(obj.get(\"headline\"), obj.get(\"name\"))\n",
    "    summary = _first_nonempty(obj.get(\"description\"), obj.get(\"alternativeHeadline\"))\n",
    "    pub = _first_nonempty(obj.get(\"datePublished\"), obj.get(\"dateModified\"))\n",
    "    return (title, summary, pub)\n",
    "\n",
    "def _parse_article_html(html: str):\n",
    "    title = \"\"\n",
    "    summary = \"\"\n",
    "    published_iso = \"\"\n",
    "    published_text = \"\"\n",
    "\n",
    "    # 1) JSON-LD\n",
    "    for m in _JSONLD_RE.finditer(html):\n",
    "        try:\n",
    "            data = json.loads(m.group(1))\n",
    "            arr = data if isinstance(data, list) else [data]\n",
    "            for d in arr:\n",
    "                t, s, p = _from_jsonld_block(d)\n",
    "                if t and not title: title = t\n",
    "                if s and not summary: summary = s\n",
    "                if p and not published_iso: published_iso = p\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # 2) Fallbacks\n",
    "    if not title:\n",
    "        title = _first_nonempty(\n",
    "            _extract_meta(html, \"og:title\"),\n",
    "            _extract_meta(html, \"twitter:title\"),\n",
    "            _extract_tag_text(html, \"title\")\n",
    "        )\n",
    "    if not summary:\n",
    "        summary = _first_nonempty(\n",
    "            _extract_meta(html, \"og:description\"),\n",
    "            _extract_meta(html, \"twitter:description\"),\n",
    "            _extract_meta(html, \"description\", attr=\"content\")\n",
    "        )\n",
    "\n",
    "    # 3) Datas\n",
    "    m_dt = re.search(r'<time[^>]+datetime=[\"\\']([^\"\\']+)[\"\\']', html, re.I)\n",
    "    if m_dt and not published_iso:\n",
    "        published_iso = m_dt.group(1)\n",
    "\n",
    "    m_txt = re.search(r'<time[^>]*>(.*?)</time>', html, re.I | re.S)\n",
    "    if m_txt:\n",
    "        published_text = re.sub(r\"<[^>]+>\", \" \", m_txt.group(1))\n",
    "        published_text = re.sub(r\"\\s+\", \" \", published_text).strip()\n",
    "\n",
    "    if not published_iso:\n",
    "        meta_pub = _extract_meta(html, \"article:published_time\")\n",
    "        if meta_pub:\n",
    "            published_iso = meta_pub\n",
    "\n",
    "    return {\n",
    "        \"title\": title.strip(),\n",
    "        \"summary\": summary.strip(),\n",
    "        \"published_at\": _norm_iso(published_iso),\n",
    "        \"published_text\": published_text.strip()\n",
    "    }\n",
    "\n",
    "async def fetch_article_details(context, url: str) -> dict:\n",
    "    try:\n",
    "        resp = await context.request.get(url, timeout=30_000)\n",
    "        if not resp.ok:\n",
    "            return {\"url\": url, \"title\":\"\", \"summary\":\"\", \"published_at\":\"\", \"published_text\":\"\"}\n",
    "        html = await resp.text()\n",
    "        data = _parse_article_html(html)\n",
    "        data[\"url\"] = url\n",
    "        data[\"collected_at\"] = _now_iso()\n",
    "        return data\n",
    "    except Exception:\n",
    "        return {\"url\": url, \"title\":\"\", \"summary\":\"\", \"published_at\":\"\", \"published_text\":\"\", \"collected_at\":_now_iso()}\n",
    "\n",
    "# ---------- SALVAR CSV ----------\n",
    "async def save_csv(rows, path=OUTCSV):\n",
    "    cols = [\"title\",\"summary\",\"url\",\"published_text\",\"published_at\",\"collected_at\"]\n",
    "    with open(path,\"w\",newline=\"\",encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=cols)\n",
    "        w.writeheader()\n",
    "        for r in rows:\n",
    "            w.writerow({k:r.get(k,\"\") for k in cols})\n",
    "    print(f\"‚úÖ CSV salvo: {path}\")\n",
    "\n",
    "# ---------- CRAWLER PRINCIPAL ----------\n",
    "async def crawl_news_articles(target: int = 100, max_visits: int = 120):\n",
    "    \"\"\"\n",
    "    Busca pelo menos 'target' artigos news/articles/<ID> navegando apenas por listagens /news.\n",
    "    Limita total de visitas (max_visits) pra evitar loop infinito.\n",
    "    \"\"\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        context = await browser.new_context()\n",
    "        page = await context.new_page()\n",
    "\n",
    "        queue = list(START_URLS)  # come√ßa por v√°rias se√ß√µes\n",
    "        visited = set()\n",
    "        found_articles = []\n",
    "        seen_articles = set()\n",
    "\n",
    "        while queue and len(found_articles) < target and len(visited) < max_visits:\n",
    "            url = queue.pop(0)\n",
    "            if url in visited: \n",
    "                continue\n",
    "            visited.add(url)\n",
    "\n",
    "            try:\n",
    "                await page.goto(url, timeout=60_000, wait_until=\"domcontentloaded\")\n",
    "                await accept_cookies(page)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            # 1) pega artigos nesta p√°gina\n",
    "            art_urls = await extract_article_urls_on_page(page)\n",
    "            for u in art_urls:\n",
    "                if u not in seen_articles:\n",
    "                    seen_articles.add(u)\n",
    "                    found_articles.append({\"url\": u, \"collected_at\": _now_iso()})\n",
    "                    if len(found_articles) >= target:\n",
    "                        break\n",
    "            if len(found_articles) >= target:\n",
    "                break\n",
    "\n",
    "            # 2) descobre novas listagens para visitar (paginadores + subse√ß√µes /news)\n",
    "            candidates = await discover_listing_pages(page)\n",
    "            for c in candidates:\n",
    "                if c not in visited and is_news_listing_url(c):\n",
    "                    queue.append(c)\n",
    "\n",
    "        # ENRIQUECIMENTO paralelizado\n",
    "        rows = found_articles[:target]\n",
    "        sem = asyncio.Semaphore(10)\n",
    "        async def worker(r):\n",
    "            async with sem:\n",
    "                info = await fetch_article_details(context, r[\"url\"])\n",
    "                r.update(info)\n",
    "        await asyncio.gather(*(worker(r) for r in rows))\n",
    "\n",
    "        await browser.close()\n",
    "        return rows\n",
    "\n",
    "# ---------- WRAPPER ----------\n",
    "async def scrape_latest_updates(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Compat:\n",
    "      - scrape_latest_updates(100)\n",
    "      - scrape_latest_updates(target=100)\n",
    "    \"\"\"\n",
    "    target = kwargs.get(\"target\", 40)\n",
    "    if args and isinstance(args[0], int):\n",
    "        target = args[0]\n",
    "    return await crawl_news_articles(target=target, max_visits=130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66e0b06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[https://www.bbc.com/news] artigos v√°lidos nesta p√°gina: 29\n",
      "[https://www.bbc.com/news] novos listings candidatos: 0\n",
      "[https://www.bbc.com/news/world] artigos v√°lidos nesta p√°gina: 19\n",
      "[https://www.bbc.com/news/world] novos listings candidatos: 0\n",
      "[https://www.bbc.com/news/uk] artigos v√°lidos nesta p√°gina: 27\n",
      "[https://www.bbc.com/news/uk] novos listings candidatos: 0\n",
      "[https://www.bbc.com/business] artigos v√°lidos nesta p√°gina: 22\n",
      "[https://www.bbc.com/business] novos listings candidatos: 0\n",
      "[https://www.bbc.com/innovation] artigos v√°lidos nesta p√°gina: 16\n",
      "[https://www.bbc.com/innovation] novos listings candidatos: 0\n",
      "[https://www.bbc.com/news/science_and_environment] artigos v√°lidos nesta p√°gina: 57\n",
      "‚úÖ CSV salvo: bbc_us_canada_latest_updates.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Execu√ß√£o\n",
    "# =========================\n",
    "data = await scrape_latest_updates(100)\n",
    "await save_csv(data, OUTCSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "261e9cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd87c51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>collected_at</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>published_at</th>\n",
       "      <th>published_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.bbc.com/news/articles/c2emmdnw82yo</td>\n",
       "      <td>2025-11-01T02:03:57.512675-03:00</td>\n",
       "      <td>Andrew Mountbatten Windsor will not leave Roya...</td>\n",
       "      <td>There are also no plans to formally remove And...</td>\n",
       "      <td>2025-10-31T20:22:09.991000+00:00</td>\n",
       "      <td>9 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.bbc.com/news/articles/cy8vrzpgxnro</td>\n",
       "      <td>2025-11-01T02:03:57.575413-03:00</td>\n",
       "      <td>Andrew: Why Sarah Ferguson, Beatrice and Eugen...</td>\n",
       "      <td>The scandal engulfing Andrew is of his own mak...</td>\n",
       "      <td>2025-10-31T19:21:48.330000+00:00</td>\n",
       "      <td>10 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.bbc.com/news/articles/cn09r01k9yqo</td>\n",
       "      <td>2025-11-01T02:03:58.789745-03:00</td>\n",
       "      <td>What Justin Trudeau&amp;#x27;s new era with Katy P...</td>\n",
       "      <td>Both the former Canadian prime minister and th...</td>\n",
       "      <td>2025-11-01T01:25:14.868000+00:00</td>\n",
       "      <td>4 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.bbc.com/news/articles/ckg4q403rpzo</td>\n",
       "      <td>2025-11-01T02:03:58.353402-03:00</td>\n",
       "      <td>Egypt&amp;#x27;s Grand Museum opens, displaying Tu...</td>\n",
       "      <td>The launch of the billion-dollar site sees fre...</td>\n",
       "      <td>2025-11-01T01:19:28.608000+00:00</td>\n",
       "      <td>4 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.bbc.com/news/articles/c0jdd186l0go</td>\n",
       "      <td>2025-11-01T02:03:57.554134-03:00</td>\n",
       "      <td>Desperation in Black River, Jamaica, after Hur...</td>\n",
       "      <td>People in Black River haven&amp;#x27;t seen any ai...</td>\n",
       "      <td>2025-11-01T02:10:13.047000+00:00</td>\n",
       "      <td>3 hours ago</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              url  \\\n",
       "0  https://www.bbc.com/news/articles/c2emmdnw82yo   \n",
       "1  https://www.bbc.com/news/articles/cy8vrzpgxnro   \n",
       "2  https://www.bbc.com/news/articles/cn09r01k9yqo   \n",
       "3  https://www.bbc.com/news/articles/ckg4q403rpzo   \n",
       "4  https://www.bbc.com/news/articles/c0jdd186l0go   \n",
       "\n",
       "                       collected_at  \\\n",
       "0  2025-11-01T02:03:57.512675-03:00   \n",
       "1  2025-11-01T02:03:57.575413-03:00   \n",
       "2  2025-11-01T02:03:58.789745-03:00   \n",
       "3  2025-11-01T02:03:58.353402-03:00   \n",
       "4  2025-11-01T02:03:57.554134-03:00   \n",
       "\n",
       "                                               title  \\\n",
       "0  Andrew Mountbatten Windsor will not leave Roya...   \n",
       "1  Andrew: Why Sarah Ferguson, Beatrice and Eugen...   \n",
       "2  What Justin Trudeau&#x27;s new era with Katy P...   \n",
       "3  Egypt&#x27;s Grand Museum opens, displaying Tu...   \n",
       "4  Desperation in Black River, Jamaica, after Hur...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  There are also no plans to formally remove And...   \n",
       "1  The scandal engulfing Andrew is of his own mak...   \n",
       "2  Both the former Canadian prime minister and th...   \n",
       "3  The launch of the billion-dollar site sees fre...   \n",
       "4  People in Black River haven&#x27;t seen any ai...   \n",
       "\n",
       "                       published_at published_text  \n",
       "0  2025-10-31T20:22:09.991000+00:00    9 hours ago  \n",
       "1  2025-10-31T19:21:48.330000+00:00   10 hours ago  \n",
       "2  2025-11-01T01:25:14.868000+00:00    4 hours ago  \n",
       "3  2025-11-01T01:19:28.608000+00:00    4 hours ago  \n",
       "4  2025-11-01T02:10:13.047000+00:00    3 hours ago  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b33ec03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   url             100 non-null    object\n",
      " 1   collected_at    100 non-null    object\n",
      " 2   title           100 non-null    object\n",
      " 3   summary         100 non-null    object\n",
      " 4   published_at    100 non-null    object\n",
      " 5   published_text  100 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 4.8+ KB\n"
     ]
    }
   ],
   "source": [
    "news.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79213c74",
   "metadata": {},
   "source": [
    "### Persist√™ncia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12fc6d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_duckdb.DuckDBPyConnection at 0x1272f6170>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 1) manter s√≥ as colunas com valor ---\n",
    "news[\"collected_at\"] = pd.to_datetime(news[\"collected_at\"], utc=True, errors=\"coerce\")\n",
    "news[\"published_at\"] = pd.to_datetime(news[\"published_at\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "news = news.drop_duplicates(subset=[\"url\"]).reset_index(drop=True)\n",
    "\n",
    "# --- 2) conectar ao banco local ---\n",
    "con = duckdb.connect(DUCKDB_PATH)\n",
    "\n",
    "# --- 3) criar a tabela de not√≠cias com apenas as colunas ---\n",
    "con.execute(\"\"\"\n",
    "DROP TABLE IF EXISTS news_bbc;\n",
    "CREATE TABLE IF NOT EXISTS news_bbc (\n",
    "    url           VARCHAR,\n",
    "    collected_at  TIMESTAMP,\n",
    "    title         VARCHAR,\n",
    "    summary       VARCHAR,\n",
    "    published_at TIMESTAMP,\n",
    "    published_text VARCHAR\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# --- 4) inserir os dados ---\n",
    "con.register(\"tmp_news\", news)\n",
    "con.execute(\"\"\"\n",
    "INSERT INTO news_bbc\n",
    "SELECT url, \n",
    "        collected_at, \n",
    "        title, \n",
    "        summary, \n",
    "        published_at,\n",
    "       published_text\n",
    "FROM tmp_news;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657870c1",
   "metadata": {},
   "source": [
    "### Verifica√ß√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a05524c",
   "metadata": {},
   "source": [
    "100 not√≠cias extra√≠das no per√≠odo de 01/08 at√© 01/11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cead3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     n\n",
      "0  100\n",
      "                                              url               collected_at  \\\n",
      "0  https://www.bbc.com/news/articles/c2emmdnw82yo 2025-11-01 02:03:57.512675   \n",
      "1  https://www.bbc.com/news/articles/cy8vrzpgxnro 2025-11-01 02:03:57.575413   \n",
      "2  https://www.bbc.com/news/articles/cn09r01k9yqo 2025-11-01 02:03:58.789745   \n",
      "3  https://www.bbc.com/news/articles/ckg4q403rpzo 2025-11-01 02:03:58.353402   \n",
      "4  https://www.bbc.com/news/articles/c0jdd186l0go 2025-11-01 02:03:57.554134   \n",
      "\n",
      "                                               title  \\\n",
      "0  Andrew Mountbatten Windsor will not leave Roya...   \n",
      "1  Andrew: Why Sarah Ferguson, Beatrice and Eugen...   \n",
      "2  What Justin Trudeau&#x27;s new era with Katy P...   \n",
      "3  Egypt&#x27;s Grand Museum opens, displaying Tu...   \n",
      "4  Desperation in Black River, Jamaica, after Hur...   \n",
      "\n",
      "                                             summary            published_at  \\\n",
      "0  There are also no plans to formally remove And... 2025-10-31 17:22:09.991   \n",
      "1  The scandal engulfing Andrew is of his own mak... 2025-10-31 16:21:48.330   \n",
      "2  Both the former Canadian prime minister and th... 2025-10-31 22:25:14.868   \n",
      "3  The launch of the billion-dollar site sees fre... 2025-10-31 22:19:28.608   \n",
      "4  People in Black River haven&#x27;t seen any ai... 2025-10-31 23:10:13.047   \n",
      "\n",
      "  published_text  \n",
      "0    9 hours ago  \n",
      "1   10 hours ago  \n",
      "2    4 hours ago  \n",
      "3    4 hours ago  \n",
      "4    3 hours ago  \n",
      "        min(published_at)       max(published_at)\n",
      "0 2025-08-05 08:36:24.551 2025-11-01 01:42:54.696\n"
     ]
    }
   ],
   "source": [
    "# --- 5) checar resultado ---\n",
    "con = duckdb.connect(DUCKDB_PATH)\n",
    "print(con.execute(\"SELECT COUNT(*) AS n FROM news_bbc\").df())\n",
    "print(con.execute(\"SELECT * FROM news_bbc LIMIT 5\").df())\n",
    "print(con.execute(\"SELECT min(published_at), max(published_at) FROM news_bbc\").df())\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d354e45",
   "metadata": {},
   "source": [
    "## API com dados de petr√≥leo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9ee6a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "\n",
    "def _retry_get(url: str, params: dict = None, max_tries: int = 5, sleep_base: float = 1.0):\n",
    "    for i in range(max_tries):\n",
    "        r = requests.get(url, params=params, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            return r\n",
    "        time.sleep(sleep_base * (2**i))\n",
    "    r.raise_for_status()\n",
    "\n",
    "def _reindex_full_range(df: pd.DataFrame, start: dt.date, end: dt.date, date_col=\"date\", value_cols=None):\n",
    "    \"\"\"Garante cobertura di√°ria START‚ÜíEND com bfill+ffill.\"\"\"\n",
    "    if value_cols is None:\n",
    "        value_cols = [c for c in df.columns if c != date_col]\n",
    "    full = pd.DataFrame({\"date\": pd.date_range(start, end, freq=\"D\").date})\n",
    "    out = full.merge(df, on=\"date\", how=\"left\")\n",
    "    # Corrige tipos num√©ricos\n",
    "    for c in value_cols:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "        out[c] = out[c].bfill().ffill()  # preenche come√ßo e meio\n",
    "    return out\n",
    "\n",
    "def fetch_fred_series_strict(series_id, start: dt.date, end: dt.date, api_key: str) -> pd.DataFrame:\n",
    "    base = \"https://api.stlouisfed.org/fred/series/observations\"\n",
    "    params = {\n",
    "        \"series_id\": series_id,\n",
    "        \"api_key\": api_key,\n",
    "        \"file_type\": \"json\",\n",
    "        \"observation_start\": start.isoformat(),\n",
    "        \"observation_end\": end.isoformat(),\n",
    "    }\n",
    "    r = _retry_get(base, params=params)\n",
    "    data = r.json().get(\"observations\", [])\n",
    "    df = pd.DataFrame(data)[[\"date\", \"value\"]] if data else pd.DataFrame(columns=[\"date\",\"value\"])\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.date\n",
    "    df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n",
    "    df = _reindex_full_range(df, start, end, value_cols=[\"value\"])\n",
    "    return df.rename(columns={\"value\": \"close\"})\n",
    "\n",
    "def fetch_coingecko_btc_strict(start: dt.date, end: dt.date, vs_currency=\"usd\") -> pd.DataFrame:\n",
    "    base = \"https://api.coingecko.com/api/v3/coins/bitcoin/market_chart\"\n",
    "    days = (end - start).days + 5\n",
    "    r = _retry_get(base, params={\"vs_currency\": vs_currency, \"days\": days})\n",
    "    js = r.json()\n",
    "    p = pd.DataFrame(js.get(\"prices\", []), columns=[\"ts_ms\", \"close\"])\n",
    "    if p.empty:\n",
    "        p = pd.DataFrame(columns=[\"date\", \"close\"])\n",
    "    else:\n",
    "        p[\"date\"] = pd.to_datetime(p[\"ts_ms\"], unit=\"ms\").dt.date\n",
    "        p = p.sort_values(\"ts_ms\").groupby(\"date\", as_index=False).tail(1)[[\"date\",\"close\"]]\n",
    "        p = p[(p[\"date\"] >= start) & (p[\"date\"] <= end)]\n",
    "    p = _reindex_full_range(p, start, end, value_cols=[\"close\"])\n",
    "    return p\n",
    "\n",
    "def ensure_min_6_months(df, start, end, date_col=\"date\"):\n",
    "    if df.empty:\n",
    "        raise AssertionError(\"DataFrame vazio.\")\n",
    "    span = (df[date_col].max() - df[date_col].min()).days\n",
    "    if span < 180:\n",
    "        raise AssertionError(f\"Menos de 6 meses: {span} dias.\")\n",
    "    # no m√°ximo 1% de buracos (ap√≥s reindex + bfill/ffill deve ser 0)\n",
    "    expected = set(pd.date_range(start, end, freq=\"D\").date)\n",
    "    got = set(df[date_col].values)\n",
    "    missing = expected - got\n",
    "    if len(missing) > len(expected) * 0.01:\n",
    "        raise AssertionError(f\"Muitas datas faltando ({len(missing)}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c33c10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coletando de 2025-04-05 at√© 2025-11-01 (~210 dias)\n",
      "[OK] BRENT: 2025-04-05 ‚Üí 2025-11-01 | 211 linhas\n",
      "[OK] GBPUSD: 2025-04-05 ‚Üí 2025-11-01 | 211 linhas\n",
      "[OK] BTCUSD: 2025-04-05 ‚Üí 2025-11-01 | 211 linhas\n",
      "           date         close   instr\n",
      "0    2025-04-05     66.130000   BRENT\n",
      "1    2025-04-06     66.130000   BRENT\n",
      "2    2025-04-07     66.130000   BRENT\n",
      "422  2025-04-05  83852.007654  BTCUSD\n",
      "423  2025-04-06  83595.885502  BTCUSD\n",
      "424  2025-04-07  78211.483582  BTCUSD\n",
      "211  2025-04-05      0.785608  GBPUSD\n",
      "212  2025-04-06      0.785608  GBPUSD\n",
      "213  2025-04-07      0.785608  GBPUSD\n",
      "           date          close   instr\n",
      "208  2025-10-30      65.520000   BRENT\n",
      "209  2025-10-31      65.520000   BRENT\n",
      "210  2025-11-01      65.520000   BRENT\n",
      "630  2025-10-30  110046.669258  BTCUSD\n",
      "631  2025-10-31  108240.765287  BTCUSD\n",
      "632  2025-11-01  110031.819531  BTCUSD\n",
      "419  2025-10-30       0.751993  GBPUSD\n",
      "420  2025-10-31       0.751993  GBPUSD\n",
      "421  2025-11-01       0.751993  GBPUSD\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Execu√ß√£o\n",
    "# =========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Coletando de {START} at√© {END} (~{(END-START).days} dias)\")\n",
    "\n",
    "    # ---- FRED: Brent & DEXUSUK ----\n",
    "    brent = fetch_fred_series_strict(\"DCOILBRENTEU\", START, END, FRED_API_KEY)  # Brent\n",
    "    dex = fetch_fred_series_strict(\"DEXUSUK\", START, END, FRED_API_KEY)         # USD/GBP\n",
    "    gbpusd = dex.assign(close=lambda d: 1.0 / d[\"close\"]).copy()                # GBP/USD\n",
    "\n",
    "    # ---- CoinGecko: BTC/USD ----\n",
    "    btc = fetch_coingecko_btc_strict(START, END, \"usd\")\n",
    "\n",
    "    # ---- Valida√ß√£o ----\n",
    "    for name, df in [(\"BRENT\", brent), (\"GBPUSD\", gbpusd), (\"BTCUSD\", btc)]:\n",
    "        ensure_min_6_months(df, START, END)\n",
    "        print(f\"[OK] {name}: {df['date'].min()} ‚Üí {df['date'].max()} | {len(df)} linhas\")\n",
    "\n",
    "    # ---- Consolida para salvar ----\n",
    "    prices = pd.concat(\n",
    "        [\n",
    "            brent.assign(instr=\"BRENT\"),\n",
    "            gbpusd.assign(instr=\"GBPUSD\"),\n",
    "            btc.assign(instr=\"BTCUSD\"),\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    ).sort_values([\"instr\", \"date\"])\n",
    "\n",
    "    print(prices.groupby(\"instr\").head(3))\n",
    "    print(prices.groupby(\"instr\").tail(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8264d8",
   "metadata": {},
   "source": [
    "### Persist√™ncia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b71b836",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(getattr(prices.index, \"dtype\", None), pd.PeriodDtype):\n",
    "    prices = prices.copy()\n",
    "    prices.index = prices.index.to_timestamp()           # para Timestamp\n",
    "    prices = prices.reset_index().rename(columns={\"index\":\"date\"})\n",
    "\n",
    "for c in prices.columns:\n",
    "    if isinstance(prices[c].dtype, pd.PeriodDtype):\n",
    "        prices[c] = prices[c].dt.to_timestamp()\n",
    "\n",
    "prices[\"date\"] = pd.to_datetime(prices[\"date\"]).dt.date   # date puro\n",
    "prices[\"instr\"] = prices[\"instr\"].astype(str)\n",
    "for c in [\"close\",\"r1\",\"r3\",\"r5\"]:\n",
    "    if c in prices:\n",
    "        prices[c] = pd.to_numeric(prices[c], errors=\"coerce\").astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a08abd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dados salvos na tabela 'prices' do banco market.duckdb\n"
     ]
    }
   ],
   "source": [
    "# salva em parquet (opcional)\n",
    "prices.to_parquet(\"prices.parquet\", index=False)\n",
    "\n",
    "# conecta ao banco local\n",
    "con = duckdb.connect(DUCKDB_PATH)\n",
    "\n",
    "# cria a tabela se n√£o existir\n",
    "con.execute(\"\"\"\n",
    "DROP TABLE IF EXISTS prices;\n",
    "CREATE TABLE IF NOT EXISTS prices (\n",
    "    date DATE,\n",
    "    close DOUBLE,\n",
    "    instr VARCHAR\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# registra o DataFrame do pandas como uma \"view\" tempor√°ria\n",
    "con.register(\"tmp_prices\", prices)\n",
    "\n",
    "# insere os dados na tabela\n",
    "con.execute(\"\"\"\n",
    "INSERT INTO prices\n",
    "SELECT * FROM tmp_prices;\n",
    "\"\"\")\n",
    "\n",
    "# confirma e fecha\n",
    "con.close()\n",
    "print(\"‚úÖ Dados salvos na tabela 'prices' do banco market.duckdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37696ab6",
   "metadata": {},
   "source": [
    "### Verifica√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1293e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instr</th>\n",
       "      <th>min_date</th>\n",
       "      <th>max_date</th>\n",
       "      <th>n_rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GBPUSD</td>\n",
       "      <td>2025-04-05</td>\n",
       "      <td>2025-11-01</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BTCUSD</td>\n",
       "      <td>2025-04-05</td>\n",
       "      <td>2025-11-01</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BRENT</td>\n",
       "      <td>2025-04-05</td>\n",
       "      <td>2025-11-01</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    instr   min_date   max_date  n_rows\n",
       "0  GBPUSD 2025-04-05 2025-11-01     211\n",
       "1  BTCUSD 2025-04-05 2025-11-01     211\n",
       "2   BRENT 2025-04-05 2025-11-01     211"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con = duckdb.connect(DUCKDB_PATH)\n",
    "\n",
    "con.execute(\"select instr, min(date) as min_date, max(date) as max_date, count(*) as n_rows from prices group by instr\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c29520",
   "metadata": {},
   "source": [
    "## Tabela intrumentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7240f045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  instr_id              symbol                           name      class\n",
      "0    BRENT        DCOILBRENTEU                   Brent (FRED)  commodity\n",
      "1   GBPUSD  DEXUSUK (inverted)  GBP/USD (from DEXUSUK ‚Äì FRED)         fx\n",
      "2   BTCUSD   CoinGecko BTC/USD            Bitcoin (CoinGecko)     crypto\n"
     ]
    }
   ],
   "source": [
    "# --- 3¬™ tabela: instruments ---\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS instruments (\n",
    "    instr_id VARCHAR PRIMARY KEY,\n",
    "    symbol   VARCHAR,\n",
    "    name     VARCHAR,\n",
    "    class    VARCHAR\n",
    ");\n",
    "\"\"\")\n",
    "con.register(\"tmp_instr\", pd.DataFrame([\n",
    "    {\"instr_id\":\"BRENT\",  \"symbol\":\"DCOILBRENTEU\",      \"name\":\"Brent (FRED)\",                  \"class\":\"commodity\"},\n",
    "    {\"instr_id\":\"GBPUSD\", \"symbol\":\"DEXUSUK (inverted)\",\"name\":\"GBP/USD (from DEXUSUK ‚Äì FRED)\", \"class\":\"fx\"},\n",
    "    {\"instr_id\":\"BTCUSD\", \"symbol\":\"CoinGecko BTC/USD\", \"name\":\"Bitcoin (CoinGecko)\",           \"class\":\"crypto\"},\n",
    "]))\n",
    "con.execute(\"DELETE FROM instruments WHERE instr_id IN (SELECT instr_id FROM tmp_instr)\")\n",
    "con.execute(\"INSERT INTO instruments SELECT * FROM tmp_instr\")\n",
    "\n",
    "print(con.execute(\"SELECT * FROM instruments\").df())\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dec70a7",
   "metadata": {},
   "source": [
    "## Banco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f1de6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          name\n",
      "0  instruments\n",
      "1     news_bbc\n",
      "2       prices\n",
      "  column_name column_type null   key default extra\n",
      "0        date        DATE  YES  None    None  None\n",
      "1       close      DOUBLE  YES  None    None  None\n",
      "2       instr     VARCHAR  YES  None    None  None\n",
      "      column_name column_type null   key default extra\n",
      "0             url     VARCHAR  YES  None    None  None\n",
      "1    collected_at   TIMESTAMP  YES  None    None  None\n",
      "2           title     VARCHAR  YES  None    None  None\n",
      "3         summary     VARCHAR  YES  None    None  None\n",
      "4    published_at   TIMESTAMP  YES  None    None  None\n",
      "5  published_text     VARCHAR  YES  None    None  None\n",
      "  column_name column_type null   key default extra\n",
      "0    instr_id     VARCHAR   NO   PRI    None  None\n",
      "1      symbol     VARCHAR  YES  None    None  None\n",
      "2        name     VARCHAR  YES  None    None  None\n",
      "3       class     VARCHAR  YES  None    None  None\n"
     ]
    }
   ],
   "source": [
    "con = duckdb.connect(DUCKDB_PATH)\n",
    "\n",
    "# lista todas as tabelas\n",
    "print(con.execute(\"SHOW TABLES\").df())\n",
    "\n",
    "# mostra o esquema completo (colunas e tipos)\n",
    "print(con.execute(\"DESCRIBE prices\").df())\n",
    "print(con.execute(\"DESCRIBE news_bbc\").df())\n",
    "print(con.execute(\"DESCRIBE instruments\").df())\n",
    "\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
